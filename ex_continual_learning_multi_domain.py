# -*- coding: utf-8 -*-
"""Copia di EX2_Continual Learning_multi-domain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UFIcQvoWLH-5O1j8Ou89587KoRkmtuef

##Ri-implementazione del secondo esercio con matrice dell'informazione##
"""

#librerie necessarie 
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import losses, optimizers, metrics
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

cross_entropy = losses.BinaryCrossentropy(from_logits=False)
adam = optimizers.Adam()
accuracy = metrics.SparseCategoricalAccuracy()

"""## Spazio delle funzioni#"""

#questa funzione trasforma la diagonale della matrice dell'informazione (tensori in vettore numpy)
def make_array(h_list):
  c1=h_list[0].numpy()
  c1=np.concatenate(c1, axis=0 )
  c2=h_list[1].numpy()
  c3=h_list[2].numpy()
  c3=c3.reshape((11,))
  d=np.concatenate((c1,c2,c3),axis=0)
  return d

# quetsa funzione calcola il termine di memoria con cui andrà aggiornata la EWC
def memory(s, theta, theta_a):
  penalty = 0
  for i, theta_i in enumerate(theta):
    _penalty = tf.math.reduce_sum(s[i]*(theta_i - theta_a[i]) ** 2)
    penalty += _penalty
  return 0.5*penalty

# questa funzione calcola la diagonale della matrice Hessiana
def diag (model):
  with tf.GradientTape(persistent=True) as tape:
    yhat=model(X)
    loss = tf.reduce_mean(tf.math.squared_difference(yhat, y))
    g_list, h_list = [], []
    for weight in model.weights:
      g = tape.gradient(loss, weight)
      g_list.append(g)
      h_list.append(tape.gradient(g, weight))
  return h_list

# questa  funzione è la custom loss-function
# uso la notazione loss_fn per tf.keras API
def ewc_loss_fn(y_true, y_pred):
    penalty = memory(c, model.trainable_variables, theta_a)
    squared_difference = tf.square(y_true - y_pred)
    return tf.reduce_mean(squared_difference, axis=-1)+penalty

# uso la notazione loss_fn per tf.keras API
def my_loss_fn(y_true, y_pred):
    squared_difference = tf.square(y_true - y_pred)
    return tf.reduce_mean(squared_difference, axis=-1)

# Backward transfer sulla matrice R come definita nelle slides
class Backwardtransfer(tf.keras.metrics.Metric):
   def __init__(self, name='Backwardtransfer', **kwargs):
     super(Backwardtransfer, self).__init__(name=name, **kwargs)
     self.backtransfer = self.add_weight(name='bkt', initializer='zeros')

   def update_state(self,X,R, sample_weight=None):
     if X is X_train_b:
       values=(R[0][0]-R[0][1])/3
     else:
       values=(R[0][0]-R[0][1]+R[0][2]-R[0][0]+R[1][2]-R[1][1])/3
     self.backtransfer.assign(values)
   
   def result(self):
     return self.backtransfer
  
   def reset_states(self):
    self.true_positives.assign(0)

"""**2.1 Split del df in tre domini**

## Preparazione dei dati##
"""

df=pd.read_csv('heart1.csv')
df_a=df[df['Age']<50]
df_p=df[df['Age']>49]
df_b=df_p[df_p['Age']<61]
df_c=df[df['Age']>60]
f_a=df_a.drop('Unnamed: 0',axis=1)
df_b=df_b.drop('Unnamed: 0',axis=1)
df_c=df_c.drop('Unnamed: 0',axis=1)
#............................................................
# preparo i dati di training e test
dataset_a=df_a.values
dataset_b=df_b.values
dataset_c=df_c.values
# scalo tutti i dati
#......................
X_a = dataset_a[:,0:11]
Y_a = dataset_a[:,11]
#......................
X_b = dataset_b[:,0:11]
Y_b = dataset_b[:,11]
#......................
X_c = dataset_c[:,0:11]
Y_c = dataset_c[:,11]
#......................

#......................
#importo la libreria per il preprocessing
from sklearn import preprocessing
#mi creo la funzione di scaling
min_max_scaler = preprocessing.MinMaxScaler()
# scalo tutti i dati per i tre dataset
X_a_scale = min_max_scaler.fit_transform(X_a)
#............................................
X_b_scale = min_max_scaler.fit_transform(X_b)
#.............................................
X_c_scale = min_max_scaler.fit_transform(X_c)
#.............................................
from sklearn.model_selection import train_test_split
#splitto il dato in 70% train 30% test
X_train_a, X_val_and_test_a, Y_train_a, Y_val_and_test_a = train_test_split(X_a_scale, Y_a, test_size=0.3)
X_val_a, X_test_a, Y_val_a, Y_test_a = train_test_split(X_val_and_test_a, Y_val_and_test_a, test_size=0.5)
#..........................................................................................................
X_train_b, X_val_and_test_b, Y_train_b, Y_val_and_test_b = train_test_split(X_b_scale, Y_b, test_size=0.3)
X_val_b, X_test_b, Y_val_b, Y_test_b = train_test_split(X_val_and_test_b, Y_val_and_test_b, test_size=0.5)
#..........................................................................................................
X_train_c, X_val_and_test_c, Y_train_c, Y_val_and_test_c = train_test_split(X_c_scale, Y_c, test_size=0.3)
X_val_c, X_test_c, Y_val_c, Y_test_c = train_test_split(X_val_and_test_c, Y_val_and_test_c, test_size=0.5)
#..........................................................................................................

"""**2.2 Modello e Training con ewc**

## Definizionedel modello e fasi di training##
"""

# definizione del modello
layer1 = tf.keras.layers.Dense(11, activation=tf.nn.relu)
layer2=tf.keras.layers.Dense(1, activation=tf.nn.relu)
model = tf.keras.models.Sequential()
model.add(layer1)
model.add(layer2)

# compilazione per il  task A
model.compile(optimizer='adam',
                  loss=my_loss_fn,
                   metrics=['accuracy'])

# training sul dataset A
model.fit(X_train_a, Y_train_a, epochs=20)

# preparo i parametri per il calcolo della matrice dell'informazione
X=X_train_a
y=Y_train_a
# Calcolo la diagonale della matrice dell'informazione
h_list=diag(model)
# trasformo la diagonale in un vettore
c=make_array(h_list)

#definisco un modello con ewc e lo valuto su B e C
#layer1 = tf.keras.layers.Dense(11, activation=tf.nn.relu)
#layer2=tf.keras.layers.Dense(1, activation=tf.nn.relu)
#ewcmodel = tf.keras.models.Sequential()
#ewcmodel.add(layer1)
#ewcmodel.add(layer2)
ewcmodel=model
#........................................................
ewcmodel.compile(optimizer='adam',
                  loss= ewc_loss_fn,
                   metrics=['accuracy'])

# estarpolo i parametri
theta_a = {n: p.value() for n, p in enumerate(model.trainable_variables.copy())}

"""##Eseguo due test su B e C##"""

a1=model.evaluate(X_val_a,Y_val_a)[1]



# valuto il modello sul task B
ewcmodel.evaluate(X_val_b,Y_val_b)

b1=ewcmodel.evaluate(X_val_b,Y_val_b)[1]

# valuto il modello sul Task C
ewcmodel.evaluate(X_val_c,Y_val_c)

c1=ewcmodel.evaluate(X_val_c,Y_val_c)[1]

"""## Traino con EWc su B e rivaluto su A##"""

# Copio le variabili del modello trainato su A sul modello con ewc
#ewcmodel.trainble_variables=model.trainable_variables

ewcmodel.fit(X_train_b, Y_train_b, epochs=20)

b2=ewcmodel.evaluate(X_val_b,Y_val_b)[1]

# Valuto su A
ewcmodel.evaluate(X_val_a,Y_val_a)

a2=ewcmodel.evaluate(X_val_a,Y_val_a)[1]

c2=ewcmodel.evaluate(X_val_c,Y_val_c)[1]

"""## Il momento della verità Traino con EWc su C e rivaluto su B ed A ##"""

# preparo i parametri per il calcolo della matrice dell'informazione
X=X_train_b
y=Y_train_b
# Calcolo la diagonale della matrice dell'informazione
h_list=diag(ewcmodel)
# trasformo la diagonale in un vettore
c=make_array(h_list)

theta_a = {n: p.value() for n, p in enumerate(ewcmodel.trainable_variables.copy())}

# training su C
ewcmodel.fit(X_train_c, Y_train_c, epochs=20)

c3=ewcmodel.evaluate(X_val_c,Y_val_c)[1]

#valuto su A
ewcmodel.evaluate(X_val_a,Y_val_a)

a3=ewcmodel.evaluate(X_val_a,Y_val_a)[1]

# valuto su B
ewcmodel.evaluate(X_val_b,Y_val_b)

b3=ewcmodel.evaluate(X_val_b,Y_val_b)[1]

"""## Risultati, matrice R_(t,t) e backward-transfer##

**2.3 Stampo i rsiultati**
"""

# Preparo i tra vettori delle performance 
va=np.array([a1,a2,a3])
vb=np.array([b1,b2,b3])
vc=np.array([c1,c2,c3])

#..............................................................
#il significato delle variabili qui è quello di rappresentare il task con le lettere ed il training task con il pedici
# e.g. b2= performace sul task b dopo allenamento su b.
#..............................................................

# Creo la matrice R_(t,t) così come indicato nelle slides
R=np.vstack([va, vb, vc])

#Calcolo i valori della backward
B1= Backwardtransfer()
B1.update_state(X_train_b,R)
bt1=float(B1.result())
bt1

B2=Backwardtransfer()
B2.update_state(X_train_c,R)
bt2=float(B2.result())
bt2

Bt=np.array([bt1,bt2])

from matplotlib import pyplot as plt

"""##Grafico##"""

values = ['Train on A', 'Train on B', 'Train on C'] 
plt.plot([1,2,3], va, label='performance on task A during training sessions',marker="o")
plt.plot([1,2,3], vb, label='performance on task B during training sessions',marker="o")
plt.plot([1,2,3], vc, label='performance on task C during training sessions',marker="o")
plt.ylabel("Model Accuracy on Tasks")
plt.xlabel("Training Sessios")
plt.xticks([1,2,3],values)
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()

R

im = plt.imshow(R, cmap='hot')
plt.colorbar(im)
plt.show()

vallues = ['Train on B', 'Train on C'] 
plt.plot([1,2], Bt, label='valori della backward tranfer',marker="o")
plt.ylabel("valori della backward tranfer")
plt.xlabel("Training Sessios")
plt.xticks([1,2],vallues)
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()